{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUB_Data_Resnet_train(IISc)_Final.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOyRX39P27AKzCeuMS+sIMo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pyrated03/IISc-Incremental-Learning/blob/main/CUB_Data_Resnet_train(IISc)_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSqwTIJR_Si4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u444TLiGiPIR"
      },
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "import os.path as osp\n",
        "import pickle\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy\n",
        "\n",
        "class CUB2002011_ds(Dataset):\n",
        "    def __init__(self, data_dir, mode=\"train\"):\n",
        "        assert mode in [\"train\", \"test\"] #Make sure the mode is one of the two\n",
        "        super(CUB2002011_ds, self).__init__() #Inheritance dataset __init__()\n",
        "        train_test_split = pd.read_csv(\n",
        "            f\"{data_dir}/train_test_split.txt\",\n",
        "            sep=\" \",\n",
        "            names=[\"img_id\", \"is_train\"],\n",
        "        )\n",
        "        img_paths = pd.read_csv(\n",
        "            f\"{data_dir}/images.txt\", sep=\" \", names=[\"img_id\", \"img_path\"]\n",
        "        )  #Read all image paths\n",
        "        labels = pd.read_csv(\n",
        "            f\"{data_dir}/image_class_labels.txt\",\n",
        "            sep=\" \",\n",
        "            names=[\"img_id\", \"img_label\"],\n",
        "        )\n",
        "        self.data_dir = osp.join(data_dir, \"images\")\n",
        "        self.mode = 1 if mode == \"train\" else 0  #Choose whether it is 1 training mode or test mode 0\n",
        "        self.img_path = img_paths.loc[\n",
        "            train_test_split[\"is_train\"] == self.mode, \"img_path\"\n",
        "        ].values  #Get the corresponding path in the corresponding mode\n",
        "        self.label = labels.loc[\n",
        "            train_test_split[\"is_train\"] == self.mode, \"img_label\"\n",
        "        ].values #Get the label in the corresponding mode\n",
        "        ############################################################################\n",
        "        split_idx = len([x for x in self.label if x<=100])\n",
        "        print(split_idx)\n",
        "        self.img_path = self.img_path[:split_idx]\n",
        "        self.label = self.label[:split_idx] \n",
        "        ############################################################################\n",
        "\n",
        "        img_sz = 84\n",
        "        if mode == 'test':\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((img_sz, img_sz)),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((img_sz, img_sz)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomVerticalFlip(),\n",
        "                transforms.RandomRotation(30),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "        self.y = self.label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.label.shape[0] #The size of the entire data set in the corresponding mode\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.transform(\n",
        "            Image.open(f\"{self.data_dir}/{self.img_path[idx]}\").convert(\"RGB\")\n",
        "        ) #Open the path of the image in the corresponding folder of the data set and convert the image data to RGB\n",
        "        label = self.label[idx]\n",
        "        # if (int)(label) <= 100:\n",
        "        return img, label-1\n",
        "        # else:\n",
        "        #   return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qo9MDthjOcK",
        "outputId": "c0a4acc4-0b46-453c-c715-4dd3995b52f8"
      },
      "source": [
        "import random\n",
        "\n",
        "data_dir = \"gdrive/My Drive/IITB Internship/CUB_200_2011\"\n",
        "train_dset = CUB2002011_ds(data_dir = data_dir, mode = \"train\")\n",
        "batch_size = 16\n",
        "  \n",
        "# l = len(train_dset)\n",
        "# split = int(np.floor(0.9 * l))\n",
        "# val_dset = train_dset[split:]\n",
        "# train_dset = train_dset[:split]\n",
        "\n",
        "size_ds = len(train_dset)\n",
        "indices = list(range(size_ds))\n",
        "split = int(np.floor(0.9 * size_ds))\n",
        "random.shuffle(indices)\n",
        "train_sampler = indices[:split]\n",
        "val_sampler = indices[split:]\n",
        "\n",
        "train_ds = DataLoader(train_dset, batch_size=batch_size, sampler=train_sampler, drop_last=True) #shuffle = True\n",
        "\n",
        "val_ds = DataLoader(train_dset, batch_size=batch_size, sampler=val_sampler, drop_last=True)#shuffle = True\n",
        "data = {'train':train_ds,'val':val_ds}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjlWbJac2A_Q",
        "outputId": "a01918e0-6d13-476a-ab28-e92ccfbbea50"
      },
      "source": [
        "test_dset = CUB2002011_ds(data_dir = data_dir, mode = \"test\")\n",
        "test_ds = DataLoader(test_dset, batch_size=batch_size, drop_last=True)#shuffle = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0_zrm2KtT1q",
        "outputId": "61dff2af-cc64-4e13-a482-d17484d104d9"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fOgzo3HtT0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7284ce87-a624-48d5-8fff-4a1c3ce76cb2"
      },
      "source": [
        "dataset_sizes = {'train':batch_size*len(train_ds), 'val':batch_size*len(val_ds),'test':batch_size*len(test_ds)}\n",
        "dataset_sizes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test': 2864, 'train': 2688, 'val': 288}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuX6kUqowySU",
        "outputId": "9bb1a23f-003f-436e-e906-2579ae01fc60"
      },
      "source": [
        "data['train']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f5ce7631ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c177a0642ef5471cb1fa9ba3835fd7a3",
            "4c75d20185864a109af21ed0335c4209",
            "ec52e73b626b4020badf0ba94cd57dc3",
            "49f81186e61a4d95b8e06b579d60d255",
            "afa10ab3252f4a91a303924efcf22803",
            "04f3572ced20445b8d0d25a16a192448",
            "03a13293ba854233bfc4aec4754d9e5c",
            "2d71bcb95d164686ae19f40427aaaad0",
            "6574ab0ed9484aa6b11e4090f476e045",
            "0574db73ff4f4800ad0c59b4265778e2",
            "3c2c3fdb786e4ad2ab38309ded35ec34"
          ]
        },
        "id": "cUzA8bwWm43i",
        "outputId": "56309536-2873-41fe-9d6e-6df558a262bb"
      },
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c177a0642ef5471cb1fa9ba3835fd7a3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDHZg8s-DEN4",
        "outputId": "2ec62da5-3ea2-4a12-a22b-c70356e74c1a"
      },
      "source": [
        "model.fc = nn.Sequential(nn.Linear(512,100))#, nn.Softmax(93))\n",
        "# model\n",
        "model = model.to(device)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=100, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ5amYDVDfF0"
      },
      "source": [
        "def train_model(model, criterion, softmax, optimizer, scheduler, device, num_epochs=5):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in data[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    # outputs = softmax(outputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    # outputs = softmax(outputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                \n",
        "                # print(\"pred: \",preds) \n",
        "                # print(\"label: \", labels.data)\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data).item()\n",
        "                # running_corrects += torch.sum(preds == labels)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / (dataset_sizes[phase])\n",
        "            epoch_acc = running_corrects*1.0 / (dataset_sizes[phase])\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_5esrkCwgub"
      },
      "source": [
        "# def train_model(model, criterion, softmax, optimizer, scheduler, device, num_epochs=5):\n",
        "#     since = time.time()\n",
        "\n",
        "#     best_model_wts = copy.deepcopy(model.state_dict())\n",
        "#     best_acc = 0.0\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "#         print('-' * 10)\n",
        "\n",
        "#         # Each epoch has a training and validation phase\n",
        "#         for phase in ['train', 'val']:\n",
        "#             if phase == 'train':\n",
        "#                 model.train()  # Set model to training mode\n",
        "#             else:\n",
        "#                 model.eval()   # Set model to evaluate mode\n",
        "\n",
        "#             running_loss = 0.0\n",
        "#             running_corrects = 0\n",
        "\n",
        "#             # Iterate over data.\n",
        "#             if phase == 'train':\n",
        "#               for inputs, labels in train_ds:\n",
        "#                   inputs = inputs.to(device)\n",
        "#                   labels = labels.to(device)\n",
        "\n",
        "#                   # zero the parameter gradients\n",
        "#                   optimizer.zero_grad()\n",
        "\n",
        "#                   # forward\n",
        "#                   # track history if only in train\n",
        "#                   with torch.set_grad_enabled(phase == 'train'):\n",
        "#                       outputs = model(inputs)\n",
        "#                       _, preds = torch.max(outputs, 1)\n",
        "#                       loss = criterion(outputs, labels)\n",
        "\n",
        "#                       # backward + optimize only if in training phase\n",
        "#                       if phase == 'train':\n",
        "#                           loss.backward()\n",
        "#                           optimizer.step()\n",
        "\n",
        "#                   # statistics\n",
        "#                   running_loss += loss.item() * inputs.size(0)\n",
        "#                   running_corrects += torch.sum(preds == labels.data)\n",
        "#               if phase == 'train':\n",
        "#                   scheduler.step()\n",
        "\n",
        "#               epoch_loss = running_loss / dataset_sizes[phase]\n",
        "#               epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "#               print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "#                   phase, epoch_loss, epoch_acc))\n",
        "              \n",
        "# ##################################################################################################################################\n",
        "#             if phase == 'val':\n",
        "#               for inputs, labels in val_ds:\n",
        "#                   inputs = inputs.to(device)\n",
        "#                   labels = labels.to(device)\n",
        "\n",
        "#                   # zero the parameter gradients\n",
        "#                   optimizer.zero_grad()\n",
        "\n",
        "#                   # forward\n",
        "#                   # track history if only in train\n",
        "#                   with torch.set_grad_enabled(phase == 'train'):\n",
        "#                       outputs = model(inputs)\n",
        "#                       _, preds = torch.max(outputs, 1)\n",
        "#                       loss = criterion(outputs, labels)\n",
        "\n",
        "#                       # backward + optimize only if in training phase\n",
        "#                       if phase == 'train':\n",
        "#                           loss.backward()\n",
        "#                           optimizer.step()\n",
        "\n",
        "#                   # statistics\n",
        "#                   running_loss += loss.item() * inputs.size(0)\n",
        "#                   running_corrects += torch.sum(preds == labels.data)\n",
        "#               if phase == 'train':\n",
        "#                   scheduler.step()\n",
        "\n",
        "#               epoch_loss = running_loss / dataset_sizes[phase]\n",
        "#               epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "#               print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "#                   phase, epoch_loss, epoch_acc))\n",
        "\n",
        "#             # deep copy the model\n",
        "#             if phase == 'val' and epoch_acc > best_acc:\n",
        "#                 best_acc = epoch_acc\n",
        "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "#         print()\n",
        "\n",
        "#     time_elapsed = time.time() - since\n",
        "#     print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "#         time_elapsed // 60, time_elapsed % 60))\n",
        "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "#     # load best model weights\n",
        "#     model.load_state_dict(best_model_wts)\n",
        "#     return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oBe5xa6xLsE"
      },
      "source": [
        "criteria = nn.CrossEntropyLoss()\n",
        "softmax = nn.Softmax()\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "# Number of epochs\n",
        "eps=75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlXAClXS9iBb"
      },
      "source": [
        "# Adam Optimizer(75 epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EMwcmW9xrpB",
        "outputId": "1dc7c4ff-2c01-44eb-fdf9-96cdaa36b427"
      },
      "source": [
        "model = train_model(model, criteria, softmax, optimizer, scheduler, device, eps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/74\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Loss: 4.3856 Acc: 0.0632\n",
            "val Loss: 3.8281 Acc: 0.1181\n",
            "\n",
            "Epoch 1/74\n",
            "----------\n",
            "train Loss: 3.2617 Acc: 0.2374\n",
            "val Loss: 3.1009 Acc: 0.2153\n",
            "\n",
            "Epoch 2/74\n",
            "----------\n",
            "train Loss: 2.6018 Acc: 0.3765\n",
            "val Loss: 2.7791 Acc: 0.2569\n",
            "\n",
            "Epoch 3/74\n",
            "----------\n",
            "train Loss: 2.1342 Acc: 0.4762\n",
            "val Loss: 2.6319 Acc: 0.3090\n",
            "\n",
            "Epoch 4/74\n",
            "----------\n",
            "train Loss: 1.7475 Acc: 0.5759\n",
            "val Loss: 2.5840 Acc: 0.2951\n",
            "\n",
            "Epoch 5/74\n",
            "----------\n",
            "train Loss: 1.4335 Acc: 0.6577\n",
            "val Loss: 2.4578 Acc: 0.3264\n",
            "\n",
            "Epoch 6/74\n",
            "----------\n",
            "train Loss: 1.1626 Acc: 0.7266\n",
            "val Loss: 2.4778 Acc: 0.3750\n",
            "\n",
            "Epoch 7/74\n",
            "----------\n",
            "train Loss: 0.9580 Acc: 0.7827\n",
            "val Loss: 2.4228 Acc: 0.3611\n",
            "\n",
            "Epoch 8/74\n",
            "----------\n",
            "train Loss: 0.7295 Acc: 0.8571\n",
            "val Loss: 2.3983 Acc: 0.4062\n",
            "\n",
            "Epoch 9/74\n",
            "----------\n",
            "train Loss: 0.6079 Acc: 0.8880\n",
            "val Loss: 2.4801 Acc: 0.3715\n",
            "\n",
            "Epoch 10/74\n",
            "----------\n",
            "train Loss: 0.4444 Acc: 0.9308\n",
            "val Loss: 2.5347 Acc: 0.3403\n",
            "\n",
            "Epoch 11/74\n",
            "----------\n",
            "train Loss: 0.3586 Acc: 0.9453\n",
            "val Loss: 2.6065 Acc: 0.3681\n",
            "\n",
            "Epoch 12/74\n",
            "----------\n",
            "train Loss: 0.2584 Acc: 0.9702\n",
            "val Loss: 2.5837 Acc: 0.3819\n",
            "\n",
            "Epoch 13/74\n",
            "----------\n",
            "train Loss: 0.2112 Acc: 0.9773\n",
            "val Loss: 2.6216 Acc: 0.3611\n",
            "\n",
            "Epoch 14/74\n",
            "----------\n",
            "train Loss: 0.1550 Acc: 0.9859\n",
            "val Loss: 2.5842 Acc: 0.3681\n",
            "\n",
            "Epoch 15/74\n",
            "----------\n",
            "train Loss: 0.1296 Acc: 0.9885\n",
            "val Loss: 2.7054 Acc: 0.3611\n",
            "\n",
            "Epoch 16/74\n",
            "----------\n",
            "train Loss: 0.0927 Acc: 0.9955\n",
            "val Loss: 2.5488 Acc: 0.3819\n",
            "\n",
            "Epoch 17/74\n",
            "----------\n",
            "train Loss: 0.0743 Acc: 0.9952\n",
            "val Loss: 2.6686 Acc: 0.3750\n",
            "\n",
            "Epoch 18/74\n",
            "----------\n",
            "train Loss: 0.0626 Acc: 0.9978\n",
            "val Loss: 2.7302 Acc: 0.3889\n",
            "\n",
            "Epoch 19/74\n",
            "----------\n",
            "train Loss: 0.0566 Acc: 0.9963\n",
            "val Loss: 2.7730 Acc: 0.3576\n",
            "\n",
            "Epoch 20/74\n",
            "----------\n",
            "train Loss: 0.0437 Acc: 0.9974\n",
            "val Loss: 2.6433 Acc: 0.3750\n",
            "\n",
            "Epoch 21/74\n",
            "----------\n",
            "train Loss: 0.0344 Acc: 0.9989\n",
            "val Loss: 2.7212 Acc: 0.3819\n",
            "\n",
            "Epoch 22/74\n",
            "----------\n",
            "train Loss: 0.0328 Acc: 0.9989\n",
            "val Loss: 2.7230 Acc: 0.3646\n",
            "\n",
            "Epoch 23/74\n",
            "----------\n",
            "train Loss: 0.0296 Acc: 1.0000\n",
            "val Loss: 2.7126 Acc: 0.3646\n",
            "\n",
            "Epoch 24/74\n",
            "----------\n",
            "train Loss: 0.0291 Acc: 0.9989\n",
            "val Loss: 2.6830 Acc: 0.3993\n",
            "\n",
            "Epoch 25/74\n",
            "----------\n",
            "train Loss: 0.0260 Acc: 0.9993\n",
            "val Loss: 2.7028 Acc: 0.4132\n",
            "\n",
            "Epoch 26/74\n",
            "----------\n",
            "train Loss: 0.0243 Acc: 0.9993\n",
            "val Loss: 2.6941 Acc: 0.3958\n",
            "\n",
            "Epoch 27/74\n",
            "----------\n",
            "train Loss: 0.0246 Acc: 1.0000\n",
            "val Loss: 2.7065 Acc: 0.3750\n",
            "\n",
            "Epoch 28/74\n",
            "----------\n",
            "train Loss: 0.0219 Acc: 1.0000\n",
            "val Loss: 2.6763 Acc: 0.3993\n",
            "\n",
            "Epoch 29/74\n",
            "----------\n",
            "train Loss: 0.0217 Acc: 1.0000\n",
            "val Loss: 2.7312 Acc: 0.4236\n",
            "\n",
            "Epoch 30/74\n",
            "----------\n",
            "train Loss: 0.0194 Acc: 1.0000\n",
            "val Loss: 2.8553 Acc: 0.3403\n",
            "\n",
            "Epoch 31/74\n",
            "----------\n",
            "train Loss: 0.0184 Acc: 1.0000\n",
            "val Loss: 2.8332 Acc: 0.3819\n",
            "\n",
            "Epoch 32/74\n",
            "----------\n",
            "train Loss: 0.0194 Acc: 0.9996\n",
            "val Loss: 2.7558 Acc: 0.3611\n",
            "\n",
            "Epoch 33/74\n",
            "----------\n",
            "train Loss: 0.0192 Acc: 0.9996\n",
            "val Loss: 2.6176 Acc: 0.4444\n",
            "\n",
            "Epoch 34/74\n",
            "----------\n",
            "train Loss: 0.0171 Acc: 1.0000\n",
            "val Loss: 2.6615 Acc: 0.3958\n",
            "\n",
            "Epoch 35/74\n",
            "----------\n",
            "train Loss: 0.0168 Acc: 1.0000\n",
            "val Loss: 2.6692 Acc: 0.3785\n",
            "\n",
            "Epoch 36/74\n",
            "----------\n",
            "train Loss: 0.0154 Acc: 1.0000\n",
            "val Loss: 2.6903 Acc: 0.4062\n",
            "\n",
            "Epoch 37/74\n",
            "----------\n",
            "train Loss: 0.0150 Acc: 0.9996\n",
            "val Loss: 2.5764 Acc: 0.4271\n",
            "\n",
            "Epoch 38/74\n",
            "----------\n",
            "train Loss: 0.0145 Acc: 1.0000\n",
            "val Loss: 2.7194 Acc: 0.4201\n",
            "\n",
            "Epoch 39/74\n",
            "----------\n",
            "train Loss: 0.0133 Acc: 1.0000\n",
            "val Loss: 2.6597 Acc: 0.3993\n",
            "\n",
            "Epoch 40/74\n",
            "----------\n",
            "train Loss: 0.0132 Acc: 1.0000\n",
            "val Loss: 2.7802 Acc: 0.3403\n",
            "\n",
            "Epoch 41/74\n",
            "----------\n",
            "train Loss: 0.0127 Acc: 1.0000\n",
            "val Loss: 2.7342 Acc: 0.3993\n",
            "\n",
            "Epoch 42/74\n",
            "----------\n",
            "train Loss: 0.0122 Acc: 1.0000\n",
            "val Loss: 2.7441 Acc: 0.3958\n",
            "\n",
            "Epoch 43/74\n",
            "----------\n",
            "train Loss: 0.0120 Acc: 1.0000\n",
            "val Loss: 2.6838 Acc: 0.3715\n",
            "\n",
            "Epoch 44/74\n",
            "----------\n",
            "train Loss: 0.0126 Acc: 0.9996\n",
            "val Loss: 2.7660 Acc: 0.3924\n",
            "\n",
            "Epoch 45/74\n",
            "----------\n",
            "train Loss: 0.0126 Acc: 1.0000\n",
            "val Loss: 2.7002 Acc: 0.3854\n",
            "\n",
            "Epoch 46/74\n",
            "----------\n",
            "train Loss: 0.0134 Acc: 0.9996\n",
            "val Loss: 2.7962 Acc: 0.3681\n",
            "\n",
            "Epoch 47/74\n",
            "----------\n",
            "train Loss: 0.0117 Acc: 1.0000\n",
            "val Loss: 2.7379 Acc: 0.3924\n",
            "\n",
            "Epoch 48/74\n",
            "----------\n",
            "train Loss: 0.0122 Acc: 1.0000\n",
            "val Loss: 2.6544 Acc: 0.3889\n",
            "\n",
            "Epoch 49/74\n",
            "----------\n",
            "train Loss: 0.0117 Acc: 1.0000\n",
            "val Loss: 2.7692 Acc: 0.3785\n",
            "\n",
            "Epoch 50/74\n",
            "----------\n",
            "train Loss: 0.0112 Acc: 1.0000\n",
            "val Loss: 2.7826 Acc: 0.3576\n",
            "\n",
            "Epoch 51/74\n",
            "----------\n",
            "train Loss: 0.0115 Acc: 1.0000\n",
            "val Loss: 2.8023 Acc: 0.3646\n",
            "\n",
            "Epoch 52/74\n",
            "----------\n",
            "train Loss: 0.0114 Acc: 1.0000\n",
            "val Loss: 2.7445 Acc: 0.3993\n",
            "\n",
            "Epoch 53/74\n",
            "----------\n",
            "train Loss: 0.0113 Acc: 0.9996\n",
            "val Loss: 2.7561 Acc: 0.3785\n",
            "\n",
            "Epoch 54/74\n",
            "----------\n",
            "train Loss: 0.0114 Acc: 1.0000\n",
            "val Loss: 2.6211 Acc: 0.4132\n",
            "\n",
            "Epoch 55/74\n",
            "----------\n",
            "train Loss: 0.0114 Acc: 1.0000\n",
            "val Loss: 2.8529 Acc: 0.4062\n",
            "\n",
            "Epoch 56/74\n",
            "----------\n",
            "train Loss: 0.0113 Acc: 1.0000\n",
            "val Loss: 2.7810 Acc: 0.4236\n",
            "\n",
            "Epoch 57/74\n",
            "----------\n",
            "train Loss: 0.0111 Acc: 1.0000\n",
            "val Loss: 2.8085 Acc: 0.3750\n",
            "\n",
            "Epoch 58/74\n",
            "----------\n",
            "train Loss: 0.0109 Acc: 1.0000\n",
            "val Loss: 2.7320 Acc: 0.3715\n",
            "\n",
            "Epoch 59/74\n",
            "----------\n",
            "train Loss: 0.0108 Acc: 0.9996\n",
            "val Loss: 2.7032 Acc: 0.3924\n",
            "\n",
            "Epoch 60/74\n",
            "----------\n",
            "train Loss: 0.0107 Acc: 1.0000\n",
            "val Loss: 2.6683 Acc: 0.3993\n",
            "\n",
            "Epoch 61/74\n",
            "----------\n",
            "train Loss: 0.0108 Acc: 1.0000\n",
            "val Loss: 2.8403 Acc: 0.4028\n",
            "\n",
            "Epoch 62/74\n",
            "----------\n",
            "train Loss: 0.0113 Acc: 1.0000\n",
            "val Loss: 2.7085 Acc: 0.3958\n",
            "\n",
            "Epoch 63/74\n",
            "----------\n",
            "train Loss: 0.0102 Acc: 1.0000\n",
            "val Loss: 2.7996 Acc: 0.3681\n",
            "\n",
            "Epoch 64/74\n",
            "----------\n",
            "train Loss: 0.0114 Acc: 0.9996\n",
            "val Loss: 2.7231 Acc: 0.3785\n",
            "\n",
            "Epoch 65/74\n",
            "----------\n",
            "train Loss: 0.0106 Acc: 1.0000\n",
            "val Loss: 2.7193 Acc: 0.3924\n",
            "\n",
            "Epoch 66/74\n",
            "----------\n",
            "train Loss: 0.0107 Acc: 1.0000\n",
            "val Loss: 2.6829 Acc: 0.3924\n",
            "\n",
            "Epoch 67/74\n",
            "----------\n",
            "train Loss: 0.0103 Acc: 1.0000\n",
            "val Loss: 2.8047 Acc: 0.3576\n",
            "\n",
            "Epoch 68/74\n",
            "----------\n",
            "train Loss: 0.0108 Acc: 1.0000\n",
            "val Loss: 2.6609 Acc: 0.3785\n",
            "\n",
            "Epoch 69/74\n",
            "----------\n",
            "train Loss: 0.0108 Acc: 1.0000\n",
            "val Loss: 2.7554 Acc: 0.3785\n",
            "\n",
            "Epoch 70/74\n",
            "----------\n",
            "train Loss: 0.0106 Acc: 1.0000\n",
            "val Loss: 2.7942 Acc: 0.3889\n",
            "\n",
            "Epoch 71/74\n",
            "----------\n",
            "train Loss: 0.0103 Acc: 1.0000\n",
            "val Loss: 2.7711 Acc: 0.3854\n",
            "\n",
            "Epoch 72/74\n",
            "----------\n",
            "train Loss: 0.0101 Acc: 1.0000\n",
            "val Loss: 2.7024 Acc: 0.3958\n",
            "\n",
            "Epoch 73/74\n",
            "----------\n",
            "train Loss: 0.0104 Acc: 1.0000\n",
            "val Loss: 2.7760 Acc: 0.4028\n",
            "\n",
            "Epoch 74/74\n",
            "----------\n",
            "train Loss: 0.0102 Acc: 1.0000\n",
            "val Loss: 2.7821 Acc: 0.3681\n",
            "\n",
            "Training complete in 74m 57s\n",
            "Best val Acc: 0.444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXiEKuh4_kLB",
        "outputId": "6638af87-3850-4e7c-a909-9d4f890e5e27"
      },
      "source": [
        "running_corrects_test = 0\n",
        "phase = \"test\"\n",
        "for input_test_final, label_test_final in test_ds:\n",
        "    input_test_final = input_test_final.float().to(device)\n",
        "    label_test_final = label_test_final.to(device)\n",
        "    # print(\"validation:\",iter_valid)\n",
        "    outputs_test_final = model(input_test_final)\n",
        "    _, preds_test_final = torch.max(outputs_test_final, 1)\n",
        "    running_corrects_test += torch.sum(preds_test_final == label_test_final.data)\n",
        "\n",
        "epoch_acc_test = running_corrects_test*1.0 / (dataset_sizes[phase])\n",
        "print(epoch_acc_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4417, device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}